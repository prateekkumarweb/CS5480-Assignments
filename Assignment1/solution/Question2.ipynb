{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000930 after 322 batches\n",
      "==> Learned function:\ty = +2.69 x^4 +1.16 x^3 -4.11 x^2 -2.46 x^1 -0.03\n",
      "==> Actual function:\ty = +2.68 x^4 +1.30 x^3 -4.10 x^2 -2.50 x^1 -0.09\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "POLY_DEGREE = 4\n",
    "W_target = torch.randn(POLY_DEGREE, 1) * 5\n",
    "b_target = torch.randn(1) * 5\n",
    "\n",
    "\n",
    "def make_features(x):\n",
    "    \"\"\"Builds features i.e. a matrix with columns [x, x^2, x^3, x^4].\"\"\"\n",
    "    x = x.unsqueeze(1)\n",
    "    return torch.cat([x ** i for i in range(1, POLY_DEGREE+1)], 1)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Approximated function.\"\"\"\n",
    "    return x.mm(W_target) + b_target[0]\n",
    "\n",
    "\n",
    "def poly_desc(W, b):\n",
    "    \"\"\"Creates a string description of a polynomial.\"\"\"\n",
    "    result = 'y = '\n",
    "    for i, w in enumerate(W):\n",
    "        result += '{:+.2f} x^{} '.format(w, len(W) - i)\n",
    "    result += '{:+.2f}'.format(b[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_batch(batch_size=32):\n",
    "    \"\"\"Builds a batch i.e. (x, f(x)) pair.\"\"\"\n",
    "    random = torch.randn(batch_size)\n",
    "    x = make_features(random)\n",
    "    y = f(x)\n",
    "    return Variable(x), Variable(y)\n",
    "\n",
    "\n",
    "# Define model\n",
    "fc = torch.nn.Linear(W_target.size(0), 1)\n",
    "\n",
    "for batch_idx in count(1):\n",
    "    # Get data\n",
    "    batch_x, batch_y = get_batch()\n",
    "\n",
    "    # Reset gradients\n",
    "    fc.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = F.smooth_l1_loss(fc(batch_x), batch_y)\n",
    "    loss = output.data[0]\n",
    "\n",
    "    # Backward pass\n",
    "    output.backward()\n",
    "\n",
    "    # Apply gradients\n",
    "    for param in fc.parameters():\n",
    "        param.data.add_(-0.1 * param.grad.data)\n",
    "\n",
    "    # Stop criterion\n",
    "    if loss < 1e-3:\n",
    "        break\n",
    "\n",
    "print('Loss: {:.6f} after {} batches'.format(loss, batch_idx))\n",
    "print('==> Learned function:\\t' + poly_desc(fc.weight.data.view(-1), fc.bias.data))\n",
    "print('==> Actual function:\\t' + poly_desc(W_target.view(-1), b_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000962 after 1928 batches with learning rate 0.01\n",
      "==> Learned function:\ty = -0.57 x^4 +0.52 x^3 -1.84 x^2 -1.19 x^1 -5.91\n",
      "==> Actual function:\ty = -0.56 x^4 +0.60 x^3 -1.85 x^2 -1.20 x^1 -5.98\n",
      "Loss: 0.000899 after 1011 batches with learning rate 0.02\n",
      "==> Learned function:\ty = -0.58 x^4 +0.54 x^3 -1.84 x^2 -1.19 x^1 -5.92\n",
      "==> Actual function:\ty = -0.56 x^4 +0.60 x^3 -1.85 x^2 -1.20 x^1 -5.98\n",
      "Loss: 0.000743 after 477 batches with learning rate 0.05\n",
      "==> Learned function:\ty = -0.55 x^4 +0.52 x^3 -1.85 x^2 -1.19 x^1 -5.93\n",
      "==> Actual function:\ty = -0.56 x^4 +0.60 x^3 -1.85 x^2 -1.20 x^1 -5.98\n",
      "Loss: 0.000685 after 438 batches with learning rate 0.1\n",
      "==> Learned function:\ty = -0.59 x^4 +0.61 x^3 -1.85 x^2 -1.22 x^1 -5.99\n",
      "==> Actual function:\ty = -0.56 x^4 +0.60 x^3 -1.85 x^2 -1.20 x^1 -5.98\n",
      "Loss: 0.000616 after 3392 batches with learning rate 0.2\n",
      "==> Learned function:\ty = -0.62 x^4 +0.55 x^3 -1.80 x^2 -1.19 x^1 -5.97\n",
      "==> Actual function:\ty = -0.56 x^4 +0.60 x^3 -1.85 x^2 -1.20 x^1 -5.98\n",
      "Loss: 0.000708 after 21443 batches with learning rate 0.3\n",
      "==> Learned function:\ty = -0.51 x^4 +0.75 x^3 -1.90 x^2 -1.28 x^1 -6.01\n",
      "==> Actual function:\ty = -0.56 x^4 +0.60 x^3 -1.85 x^2 -1.20 x^1 -5.98\n",
      "Loss: 0.000612 after 152141 batches with learning rate 0.5\n",
      "==> Learned function:\ty = -0.59 x^4 +0.57 x^3 -1.90 x^2 -1.22 x^1 -6.00\n",
      "==> Actual function:\ty = -0.56 x^4 +0.60 x^3 -1.85 x^2 -1.20 x^1 -5.98\n",
      "Loss: 0.000851 after 835801 batches with learning rate 0.7\n",
      "==> Learned function:\ty = -0.64 x^4 +0.69 x^3 -1.77 x^2 -1.23 x^1 -6.01\n",
      "==> Actual function:\ty = -0.56 x^4 +0.60 x^3 -1.85 x^2 -1.20 x^1 -5.98\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "POLY_DEGREE = 4\n",
    "W_target = torch.randn(POLY_DEGREE, 1) * 5\n",
    "b_target = torch.randn(1) * 5\n",
    "\n",
    "\n",
    "def make_features(x):\n",
    "    \"\"\"Builds features i.e. a matrix with columns [x, x^2, x^3, x^4].\"\"\"\n",
    "    x = x.unsqueeze(1)\n",
    "    return torch.cat([x ** i for i in range(1, POLY_DEGREE+1)], 1)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Approximated function.\"\"\"\n",
    "    return x.mm(W_target) + b_target[0]\n",
    "\n",
    "\n",
    "def poly_desc(W, b):\n",
    "    \"\"\"Creates a string description of a polynomial.\"\"\"\n",
    "    result = 'y = '\n",
    "    for i, w in enumerate(W):\n",
    "        result += '{:+.2f} x^{} '.format(w, len(W) - i)\n",
    "    result += '{:+.2f}'.format(b[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(batch_size=32):\n",
    "    \"\"\"Builds a batch i.e. (x, f(x)) pair.\"\"\"\n",
    "    random = torch.randn(batch_size)\n",
    "    x = make_features(random)\n",
    "    y = f(x)\n",
    "    return Variable(x), Variable(y)\n",
    "\n",
    "def run(rate):\n",
    "    # Define model\n",
    "    fc = torch.nn.Linear(W_target.size(0), 1)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.SGD(fc.parameters(), lr=rate)\n",
    "\n",
    "    for batch_idx in count(1):\n",
    "        # Get data\n",
    "        batch_x, batch_y = get_batch()\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = F.smooth_l1_loss(fc(batch_x), batch_y)\n",
    "        loss = output.data[0]\n",
    "\n",
    "        # Backward pass\n",
    "        output.backward()\n",
    "\n",
    "        # Apply gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Stop criterion\n",
    "        if loss < 1e-3:\n",
    "            break\n",
    "\n",
    "    print('Loss: {:.6f} after {} batches with learning rate {}'.format(loss, batch_idx, rate))\n",
    "    print('==> Learned function:\\t' + poly_desc(fc.weight.data.view(-1), fc.bias.data))\n",
    "    print('==> Actual function:\\t' + poly_desc(W_target.view(-1), b_target))\n",
    "\n",
    "run(0.01)\n",
    "run(0.02)\n",
    "run(0.05)\n",
    "run(0.1)\n",
    "run(0.2)\n",
    "run(0.3)\n",
    "run(0.5)\n",
    "run(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I observe that if we have learning rate too low, then it takes more batches to train the model. Also if we have learning rate too high, it takes more baches to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.599692 after 24992 batches with learning rate 0.01\n",
      "Learned weights: 0.812789 0.945869 and bias: 31.062693\n",
      "\n",
      " 39.7229\n",
      " 43.9199\n",
      " 50.0087\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from itertools import count\n",
    "\n",
    "train_data = np.genfromtxt('qn2_data.csv',delimiter=',')\n",
    "train_data = torch.from_numpy(train_data)\n",
    "train_data = train_data.type(torch.FloatTensor)\n",
    "train_X = train_data[:,:-1]\n",
    "train_y = train_data[:,-1]\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def get_batch(batch_size=2):\n",
    "    batch_X = torch.Tensor(batch_size, train_X.size(1))\n",
    "    batch_y = torch.Tensor(batch_size, 1)\n",
    "    for i in range(batch_size):\n",
    "        idx = np.random.randint(0, train_X.size(0))\n",
    "        batch_X[i] = train_X[idx]\n",
    "        batch_y[i] = train_y[idx]\n",
    "    return Variable(batch_X), Variable(batch_y)\n",
    "\n",
    "def train(rate):\n",
    "    fc = torch.nn.Linear(train_X.size(1), 1)\n",
    "    optimizer = torch.optim.SGD(fc.parameters(), lr=rate)\n",
    "    loss = 0\n",
    "    for batch_idx in count(1):\n",
    "        \n",
    "        batch_X, batch_y = get_batch(2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = F.smooth_l1_loss(fc(batch_X), batch_y)\n",
    "        loss = output.data[0]\n",
    "        \n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = F.smooth_l1_loss(fc(Variable(train_X)), Variable(train_y)).data[0]\n",
    "        if loss < 0.6:\n",
    "            break\n",
    "    print('Loss: {:.6f} after {} batches with learning rate {}'.format(loss, batch_idx, rate))\n",
    "    weights = fc.weight.data.view(-1)\n",
    "    bias = fc.bias.data\n",
    "    print('Learned weights: {:.6f} {:.6f} and bias: {:.6f}'.format(weights[0], weights[1], bias[0]))\n",
    "    return fc\n",
    "\n",
    "model = train(0.01)\n",
    "test_set = torch.Tensor([[6,4],[10,5],[14,8]])\n",
    "test_set = Variable(test_set)\n",
    "test_y = model(test_set)\n",
    "print(test_y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights\n",
    "0.812789\n",
    "0.945869\n",
    "\n",
    "#### Bias\n",
    "31.062693\n",
    "\n",
    "#### Predictions\n",
    "\n",
    "\n",
    "| Fertilizer | Insecticide | Corn produced |\n",
    "|---|---|---|\n",
    "| 6 | 4 | 39.7229 |\n",
    "| 10 | 5 | 43.9199 |\n",
    "| 14 | 8 | 50.0087 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights: 0.650083 1.109883 and bias: 31.980692\n",
      "\n",
      " 40.3207\n",
      " 44.0309\n",
      " 49.9609\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from itertools import count\n",
    "\n",
    "train_data = np.genfromtxt('qn2_data.csv',delimiter=',')\n",
    "train_data = torch.from_numpy(train_data)\n",
    "train_data = train_data.type(torch.FloatTensor)\n",
    "X = train_data[:,:-1]\n",
    "y = train_data[:,-1]\n",
    "\n",
    "X = torch.cat((X, torch.ones(X.size(0), 1)), dim=1)\n",
    "\n",
    "theta = torch.mm(torch.mm(X.transpose(0, 1), X).inverse(), torch.mm(X.transpose(0, 1), y.unsqueeze(1)))\n",
    "theta1 = theta.view(-1)\n",
    "print('Learned weights: {:.6f} {:.6f} and bias: {:.6f}'.format(theta1[0], theta1[1], theta1[2]))\n",
    "\n",
    "test_X = torch.Tensor([[6,4],[10,5],[14,8]])\n",
    "test_X = torch.cat((test_X, torch.ones(test_X.size(0), 1)), dim=1)\n",
    "print(torch.mm(test_X, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights\n",
    "0.650083\n",
    "1.109883\n",
    "\n",
    "#### Bias\n",
    "31.980692\n",
    "\n",
    "#### Predictions\n",
    "\n",
    "\n",
    "| Fertilizer | Insecticide | Corn produced |\n",
    "|---|---|---|\n",
    "| 6 | 4 | 40.3207 |\n",
    "| 10 | 5 | 44.0309 |\n",
    "| 14 | 8 | 49.9609 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and bias learnt by the linear model are approximately same to that of least squares solution. Even the predictions are similar to that of least squares solution. The first weight learned is higher than that of least squares and the second weight learned is lower than that of least squares. The bias is approximately same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
