{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.366173\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.615206\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.805127\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.566315\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.235285\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.488180\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.220668\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.260928\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.236319\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.316104\n",
      "\n",
      "Train set: Average loss: 0.1642, Accuracy: 57093/60000 (95%)\n",
      "Test set: Average loss: 0.1544, Accuracy: 9550/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.396924\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.169391\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.138631\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.187013\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.300367\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.347784\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.227688\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.191856\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.186168\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.121868\n",
      "\n",
      "Train set: Average loss: 0.1043, Accuracy: 58058/60000 (97%)\n",
      "Test set: Average loss: 0.1019, Accuracy: 9696/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.161127\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.133642\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.216266\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.274225\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.147754\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.162891\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.167611\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.325684\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.114349\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.097950\n",
      "\n",
      "Train set: Average loss: 0.0785, Accuracy: 58549/60000 (98%)\n",
      "Test set: Average loss: 0.0748, Accuracy: 9773/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.055535\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.168452\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.157682\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.085073\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.234850\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.060951\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.213123\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.049780\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.237254\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.065991\n",
      "\n",
      "Train set: Average loss: 0.0631, Accuracy: 58839/60000 (98%)\n",
      "Test set: Average loss: 0.0635, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.136333\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.078949\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.040683\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.035096\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.088538\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.294565\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.045970\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.093449\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.244499\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.269068\n",
      "\n",
      "Train set: Average loss: 0.0617, Accuracy: 58855/60000 (98%)\n",
      "Test set: Average loss: 0.0591, Accuracy: 9817/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.014892\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.102878\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.054316\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.163386\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.052739\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.066810\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.149891\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.025748\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.009024\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.091817\n",
      "\n",
      "Train set: Average loss: 0.0493, Accuracy: 59111/60000 (99%)\n",
      "Test set: Average loss: 0.0504, Accuracy: 9839/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.081155\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.022765\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.046514\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.042030\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.067411\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.139681\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.053668\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.033775\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.051918\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.173301\n",
      "\n",
      "Train set: Average loss: 0.0448, Accuracy: 59188/60000 (99%)\n",
      "Test set: Average loss: 0.0471, Accuracy: 9844/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.136043\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.280676\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.113722\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.015215\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.090284\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.042848\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.066153\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.111830\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.041736\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.050176\n",
      "\n",
      "Train set: Average loss: 0.0414, Accuracy: 59241/60000 (99%)\n",
      "Test set: Average loss: 0.0413, Accuracy: 9858/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.047364\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.027602\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.077486\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.082947\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.034678\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.023205\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.113574\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.174995\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.186708\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.037896\n",
      "\n",
      "Train set: Average loss: 0.0376, Accuracy: 59305/60000 (99%)\n",
      "Test set: Average loss: 0.0399, Accuracy: 9872/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.096060\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.043923\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.111392\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.124354\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.045669\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.082536\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.029656\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.016947\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.051396\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.133440\n",
      "\n",
      "Train set: Average loss: 0.0364, Accuracy: 59330/60000 (99%)\n",
      "Test set: Average loss: 0.0382, Accuracy: 9874/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "args_batch_size = 64\n",
    "args_test_batch_size = 1000\n",
    "args_epochs = 10\n",
    "args_lr = 0.01\n",
    "args_momentum = 0.5\n",
    "args_no_cuda = False\n",
    "args_seed = 1\n",
    "args_log_interval = 100\n",
    "\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args_seed)\n",
    "if args_cuda:\n",
    "    torch.cuda.manual_seed(args_seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(p=p)\n",
    "        self.p = p\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training, p=self.p)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net(p=0.25)\n",
    "if args_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args_lr, momentum=args_momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for data, target in train_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        train_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        train_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "    for data, target in test_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        test_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset),\n",
    "        100. * train_correct / len(train_loader.dataset)))\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, test_correct, len(test_loader.dataset),\n",
    "        100. * test_correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, args_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.376790\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.004478\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.205587\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.931227\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.563367\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.660168\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.491314\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.367745\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.529450\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.554864\n",
      "\n",
      "Train set: Average loss: 0.2207, Accuracy: 56179/60000 (94%)\n",
      "Test set: Average loss: 0.2073, Accuracy: 9415/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.570426\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.349697\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.319366\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.324545\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.402807\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.331285\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.405334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.223397\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.490819\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.296813\n",
      "\n",
      "Train set: Average loss: 0.1342, Accuracy: 57604/60000 (96%)\n",
      "Test set: Average loss: 0.1283, Accuracy: 9616/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.466176\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.229794\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.378948\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.365149\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.203575\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.314042\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.302099\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.469508\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.247803\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.236908\n",
      "\n",
      "Train set: Average loss: 0.1036, Accuracy: 58121/60000 (97%)\n",
      "Test set: Average loss: 0.0975, Accuracy: 9698/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.088127\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.294467\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.181525\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.158760\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.378383\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.194548\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.297544\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.130493\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.341912\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.158562\n",
      "\n",
      "Train set: Average loss: 0.0868, Accuracy: 58454/60000 (97%)\n",
      "Test set: Average loss: 0.0814, Accuracy: 9758/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.183381\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.264723\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.089933\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.060440\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.105376\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.411569\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.086999\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.077902\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.431585\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.603942\n",
      "\n",
      "Train set: Average loss: 0.0765, Accuracy: 58619/60000 (98%)\n",
      "Test set: Average loss: 0.0707, Accuracy: 9787/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.148552\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.324967\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.134734\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.371249\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.161026\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.158198\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.366838\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.104308\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.042223\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.243791\n",
      "\n",
      "Train set: Average loss: 0.0674, Accuracy: 58791/60000 (98%)\n",
      "Test set: Average loss: 0.0652, Accuracy: 9799/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.152316\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.100765\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.159656\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.229851\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.182118\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.121811\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.239991\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.094872\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.250804\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.352546\n",
      "\n",
      "Train set: Average loss: 0.0646, Accuracy: 58836/60000 (98%)\n",
      "Test set: Average loss: 0.0619, Accuracy: 9813/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.148663\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.508446\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.215514\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.228121\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.133872\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.139619\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.057283\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.330353\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.129765\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.110693\n",
      "\n",
      "Train set: Average loss: 0.0601, Accuracy: 58921/60000 (98%)\n",
      "Test set: Average loss: 0.0550, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.127779\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.088449\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.093187\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.232992\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.117561\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.108622\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.186231\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.278376\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.416018\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.085192\n",
      "\n",
      "Train set: Average loss: 0.0579, Accuracy: 58930/60000 (98%)\n",
      "Test set: Average loss: 0.0536, Accuracy: 9830/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.169865\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.248154\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.233860\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.117223\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.103420\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.243261\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.076231\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.114411\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.204152\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.230770\n",
      "\n",
      "Train set: Average loss: 0.0555, Accuracy: 59007/60000 (98%)\n",
      "Test set: Average loss: 0.0539, Accuracy: 9842/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "args_batch_size = 64\n",
    "args_test_batch_size = 1000\n",
    "args_epochs = 10\n",
    "args_lr = 0.01\n",
    "args_momentum = 0.5\n",
    "args_no_cuda = False\n",
    "args_seed = 1\n",
    "args_log_interval = 100\n",
    "\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args_seed)\n",
    "if args_cuda:\n",
    "    torch.cuda.manual_seed(args_seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(p=p)\n",
    "        self.p = p\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training, p=self.p)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net(p=0.5)\n",
    "if args_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args_lr, momentum=args_momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for data, target in train_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        train_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        train_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "    for data, target in test_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        test_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset),\n",
    "        100. * train_correct / len(train_loader.dataset)))\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, test_correct, len(test_loader.dataset),\n",
    "        100. * test_correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, args_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.457130\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.252684\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.112433\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.897566\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.612304\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.646706\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.108676\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.059297\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.247025\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.234648\n",
      "\n",
      "Train set: Average loss: 0.4539, Accuracy: 53497/60000 (89%)\n",
      "Test set: Average loss: 0.4329, Accuracy: 8986/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.114151\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.988425\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.893794\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.826047\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.135631\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.638794\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.851916\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.922302\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.820934\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.998640\n",
      "\n",
      "Train set: Average loss: 0.2517, Accuracy: 56012/60000 (93%)\n",
      "Test set: Average loss: 0.2314, Accuracy: 9410/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.987001\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.932370\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.807556\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.888542\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.563658\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.704419\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.872582\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.025174\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.640001\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.787420\n",
      "\n",
      "Train set: Average loss: 0.2050, Accuracy: 56885/60000 (95%)\n",
      "Test set: Average loss: 0.1864, Accuracy: 9527/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.513499\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.782578\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.666012\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.745432\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.723215\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.643685\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.868494\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.422744\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.773780\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.605079\n",
      "\n",
      "Train set: Average loss: 0.1684, Accuracy: 57238/60000 (95%)\n",
      "Test set: Average loss: 0.1502, Accuracy: 9583/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.572377\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.611470\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.596975\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.559422\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.489909\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.957190\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.452739\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.571731\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.794154\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.695889\n",
      "\n",
      "Train set: Average loss: 0.1456, Accuracy: 57530/60000 (96%)\n",
      "Test set: Average loss: 0.1278, Accuracy: 9636/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.712657\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.583565\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.368053\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.680140\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.621744\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.718866\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.685088\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.539244\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.579245\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.966153\n",
      "\n",
      "Train set: Average loss: 0.1329, Accuracy: 57704/60000 (96%)\n",
      "Test set: Average loss: 0.1198, Accuracy: 9646/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.447545\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.608542\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.666289\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.578092\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.476252\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.510206\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.621016\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.531851\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.671780\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.862150\n",
      "\n",
      "Train set: Average loss: 0.1240, Accuracy: 57889/60000 (96%)\n",
      "Test set: Average loss: 0.1105, Accuracy: 9695/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.539539\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.822074\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.544077\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.526079\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.529732\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.676678\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.412451\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.833326\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.458350\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.513989\n",
      "\n",
      "Train set: Average loss: 0.1187, Accuracy: 57913/60000 (97%)\n",
      "Test set: Average loss: 0.1084, Accuracy: 9690/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.705170\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.425415\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.593521\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.548252\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.506674\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.507837\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.776182\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.846178\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.798346\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.435714\n",
      "\n",
      "Train set: Average loss: 0.1084, Accuracy: 58082/60000 (97%)\n",
      "Test set: Average loss: 0.0992, Accuracy: 9691/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.752427\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.510098\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.523192\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.516257\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.696407\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.704118\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.395014\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.440933\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.652620\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.481455\n",
      "\n",
      "Train set: Average loss: 0.1075, Accuracy: 58105/60000 (97%)\n",
      "Test set: Average loss: 0.0964, Accuracy: 9706/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "args_batch_size = 64\n",
    "args_test_batch_size = 1000\n",
    "args_epochs = 10\n",
    "args_lr = 0.01\n",
    "args_momentum = 0.5\n",
    "args_no_cuda = False\n",
    "args_seed = 1\n",
    "args_log_interval = 100\n",
    "\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args_seed)\n",
    "if args_cuda:\n",
    "    torch.cuda.manual_seed(args_seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(p=p)\n",
    "        self.p = p\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training, p=self.p)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net(p=0.75)\n",
    "if args_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args_lr, momentum=args_momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for data, target in train_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        train_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        train_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "    for data, target in test_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        test_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset),\n",
    "        100. * train_correct / len(train_loader.dataset)))\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, test_correct, len(test_loader.dataset),\n",
    "        100. * test_correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, args_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307839\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.306502\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.301817\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.310714\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.304682\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.297040\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.312536\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.296304\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.305298\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.304569\n",
      "\n",
      "Train set: Average loss: 2.3270, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3282, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.309081\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.285652\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.307020\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.305887\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.290152\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.295799\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.307236\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.296850\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.306014\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.301690\n",
      "\n",
      "Train set: Average loss: 2.3271, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3283, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.289872\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.287848\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.294832\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.300841\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.305375\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.293122\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.305744\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.301871\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.293492\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.301281\n",
      "\n",
      "Train set: Average loss: 2.3271, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3283, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.306454\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.298285\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.301688\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.311983\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.294261\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.308790\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.312335\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.307649\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.304770\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.315787\n",
      "\n",
      "Train set: Average loss: 2.3269, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3281, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.310983\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.316926\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.305415\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.300359\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.302447\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.300312\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.308800\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.310294\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.291393\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.309399\n",
      "\n",
      "Train set: Average loss: 2.3268, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3279, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.290354\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.282600\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.309471\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.307001\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.287114\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.302312\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.291502\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 2.290036\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.296258\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 2.303176\n",
      "\n",
      "Train set: Average loss: 2.3266, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3278, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.293282\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 2.307009\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.314291\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 2.307268\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.318128\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.290037\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.301353\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 2.302610\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.290082\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 2.302787\n",
      "\n",
      "Train set: Average loss: 2.3268, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3278, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.304442\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 2.302392\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.301334\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 2.288649\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.291445\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.300092\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.308568\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 2.302121\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.296927\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.312535\n",
      "\n",
      "Train set: Average loss: 2.3269, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3280, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.308550\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 2.300853\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.301188\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 2.303457\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.298369\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.302411\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.289859\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 2.302822\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.296550\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 2.294971\n",
      "\n",
      "Train set: Average loss: 2.3272, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3284, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.308598\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 2.305953\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 2.297743\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 2.303738\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 2.294244\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 2.303448\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 2.301202\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 2.303486\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 2.296977\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 2.292408\n",
      "\n",
      "Train set: Average loss: 2.3272, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3283, Accuracy: 1135/10000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "args_batch_size = 64\n",
    "args_test_batch_size = 1000\n",
    "args_epochs = 10\n",
    "args_lr = 0.01\n",
    "args_momentum = 0.5\n",
    "args_no_cuda = False\n",
    "args_seed = 1\n",
    "args_log_interval = 100\n",
    "\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args_seed)\n",
    "if args_cuda:\n",
    "    torch.cuda.manual_seed(args_seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(p=p)\n",
    "        self.p = p\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training, p=self.p)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net(p=1.0)\n",
    "if args_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args_lr, momentum=args_momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for data, target in train_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        train_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        train_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "    for data, target in test_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        test_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset),\n",
    "        100. * train_correct / len(train_loader.dataset)))\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, test_correct, len(test_loader.dataset),\n",
    "        100. * test_correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, args_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set: Average loss: 0.0364, Accuracy: 59330/60000 (99%)\n",
    "Test set: Average loss: 0.0382, Accuracy: 9874/10000 (99%)\n",
    "\n",
    "Train set: Average loss: 0.0555, Accuracy: 59007/60000 (98%)\n",
    "Test set: Average loss: 0.0539, Accuracy: 9842/10000 (98%)\n",
    "\n",
    "Train set: Average loss: 0.1075, Accuracy: 58105/60000 (97%)\n",
    "Test set: Average loss: 0.0964, Accuracy: 9706/10000 (97%)\n",
    "\n",
    "Train set: Average loss: 2.3272, Accuracy: 6742/60000 (11%)\n",
    "Test set: Average loss: 2.3283, Accuracy: 1135/10000 (11%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "|Dropout probability|Train performance|Test performance|\n",
    "|---|---|---|\n",
    "|0.25|59330/60000 (99%)|9874/10000 (99%)|\n",
    "|0.50|59007/60000 (98%)|9842/10000 (98%)|\n",
    "|0.75|58105/60000 (97%)|9706/10000 (97%)|\n",
    "|1.00|6742/60000 (11%)|1135/10000 (11%)|\n",
    "\n",
    "* As the dropout probability increases the performance is decreasing\n",
    "* In general, if dropout probability is neither too lo nor too high, then it is expected to give better performance, but in this case the performance is decreasing.\n",
    "* I think it is the case that the dropout proability where high performance is present is below 0.25\n",
    "* Dropout probability being 1 is effictively removing a layer of network. Hence this performs worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
