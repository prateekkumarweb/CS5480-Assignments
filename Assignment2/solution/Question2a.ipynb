{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.376790\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.004478\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.205587\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.931227\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.563367\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.660168\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.491314\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.367745\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.529450\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.554864\n",
      "\n",
      "Train set: Average loss: 0.2207, Accuracy: 56179/60000 (94%)\n",
      "Test set: Average loss: 0.2073, Accuracy: 9415/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.570426\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.349697\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.319366\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.324545\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.402807\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.331285\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.405334\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.223397\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.490819\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.296813\n",
      "\n",
      "Train set: Average loss: 0.1342, Accuracy: 57604/60000 (96%)\n",
      "Test set: Average loss: 0.1283, Accuracy: 9616/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.466176\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.229794\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.378948\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.365149\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.203575\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.314042\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.302099\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.469508\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.247803\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.236908\n",
      "\n",
      "Train set: Average loss: 0.1036, Accuracy: 58121/60000 (97%)\n",
      "Test set: Average loss: 0.0975, Accuracy: 9698/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.088127\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.294467\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.181525\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.158760\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.378383\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.194548\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.297544\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.130493\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.341912\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.158562\n",
      "\n",
      "Train set: Average loss: 0.0868, Accuracy: 58454/60000 (97%)\n",
      "Test set: Average loss: 0.0814, Accuracy: 9758/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.183381\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.264723\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.089933\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.060440\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.105376\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.411569\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.086999\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.077902\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.431585\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.603942\n",
      "\n",
      "Train set: Average loss: 0.0765, Accuracy: 58619/60000 (98%)\n",
      "Test set: Average loss: 0.0707, Accuracy: 9787/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.148552\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.324967\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.134734\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.371249\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.161026\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.158198\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.366838\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.104308\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.042223\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.243791\n",
      "\n",
      "Train set: Average loss: 0.0674, Accuracy: 58791/60000 (98%)\n",
      "Test set: Average loss: 0.0652, Accuracy: 9799/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.152316\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.100765\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.159656\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.229851\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.182118\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.121811\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.239991\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.094872\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.250804\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.352546\n",
      "\n",
      "Train set: Average loss: 0.0646, Accuracy: 58836/60000 (98%)\n",
      "Test set: Average loss: 0.0619, Accuracy: 9813/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.148663\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.508446\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.215514\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.228121\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.133872\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.139619\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.057283\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.330353\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.129765\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.110693\n",
      "\n",
      "Train set: Average loss: 0.0601, Accuracy: 58921/60000 (98%)\n",
      "Test set: Average loss: 0.0550, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.127779\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.088449\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.093187\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.232992\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.117561\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.108622\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.186231\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.278376\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.416018\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.085192\n",
      "\n",
      "Train set: Average loss: 0.0579, Accuracy: 58930/60000 (98%)\n",
      "Test set: Average loss: 0.0536, Accuracy: 9830/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.169865\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.248154\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.233860\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.117223\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.103420\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.243261\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.076231\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.114411\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.204152\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.230770\n",
      "\n",
      "Train set: Average loss: 0.0555, Accuracy: 59007/60000 (98%)\n",
      "Test set: Average loss: 0.0539, Accuracy: 9842/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "args_batch_size = 64\n",
    "args_test_batch_size = 1000\n",
    "args_epochs = 10\n",
    "args_lr = 0.01\n",
    "args_momentum = 0.5\n",
    "args_no_cuda = False\n",
    "args_seed = 1\n",
    "args_log_interval = 100\n",
    "\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args_seed)\n",
    "if args_cuda:\n",
    "    torch.cuda.manual_seed(args_seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "if args_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args_lr, momentum=args_momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for data, target in train_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        train_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        train_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "    for data, target in test_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        test_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset),\n",
    "        100. * train_correct / len(train_loader.dataset)))\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, test_correct, len(test_loader.dataset),\n",
    "        100. * test_correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, args_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.378154\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.326673\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.337831\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.348187\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.300359\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.303439\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.305567\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.319268\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.315299\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.327421\n",
      "\n",
      "Train set: Average loss: 2.3009, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3009, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.303193\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.300387\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.298056\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.290768\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.282754\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.318167\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.285486\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.296819\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.314660\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.321749\n",
      "\n",
      "Train set: Average loss: 2.3002, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.3003, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.319182\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.311701\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.297778\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.295753\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.276694\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.305433\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.288033\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.311194\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.290170\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.301033\n",
      "\n",
      "Train set: Average loss: 2.3000, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.2999, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.283669\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.297302\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.312001\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.319410\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.293856\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.299936\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.319128\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.284231\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.291708\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.323274\n",
      "\n",
      "Train set: Average loss: 2.2994, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.2992, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.301384\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.286387\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.291279\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.282529\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.284405\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.319218\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.312584\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.294097\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.293846\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.292723\n",
      "\n",
      "Train set: Average loss: 2.2988, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.2986, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.330063\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.303277\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.281329\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.299641\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.294749\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.312414\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.297855\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 2.284344\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.310644\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 2.299757\n",
      "\n",
      "Train set: Average loss: 2.2979, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.2977, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.307599\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 2.303472\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.301049\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 2.288422\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.300886\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.300565\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.292603\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 2.304775\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.300345\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 2.295241\n",
      "\n",
      "Train set: Average loss: 2.2958, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.2955, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.292369\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 2.293200\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.298715\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 2.289074\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.299478\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.298201\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.305589\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 2.286698\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.297768\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.277824\n",
      "\n",
      "Train set: Average loss: 2.2902, Accuracy: 6742/60000 (11%)\n",
      "Test set: Average loss: 2.2897, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.307752\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 2.297654\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.285003\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 2.271258\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.268547\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.304325\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.272212\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 2.296296\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.260641\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 2.281384\n",
      "\n",
      "Train set: Average loss: 2.2695, Accuracy: 11902/60000 (20%)\n",
      "Test set: Average loss: 2.2687, Accuracy: 2029/10000 (20%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.278417\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 2.289983\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 2.252113\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 2.261831\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 2.246394\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 2.263672\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 2.277814\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 2.228803\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 2.218137\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 2.182414\n",
      "\n",
      "Train set: Average loss: 2.1522, Accuracy: 16994/60000 (28%)\n",
      "Test set: Average loss: 2.1483, Accuracy: 2814/10000 (28%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "args_batch_size = 64\n",
    "args_test_batch_size = 1000\n",
    "args_epochs = 10\n",
    "args_lr = 0.01\n",
    "args_momentum = 0.5\n",
    "args_no_cuda = False\n",
    "args_seed = 1\n",
    "args_log_interval = 100\n",
    "\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args_seed)\n",
    "if args_cuda:\n",
    "    torch.cuda.manual_seed(args_seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.sigmoid(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "if args_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args_lr, momentum=args_momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for data, target in train_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        train_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        train_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "    for data, target in test_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        test_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset),\n",
    "        100. * train_correct / len(train_loader.dataset)))\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, test_correct, len(test_loader.dataset),\n",
    "        100. * test_correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, args_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "* Using ReLU\n",
    "  * Train Performance: 59007/60000 (98%)\n",
    "  * Test Performance: 9842/10000 (98%)\n",
    "* Using Sigmoid\n",
    "  * Train Performance: 16994/60000 (28%)\n",
    "  * Test Performance: 2814/10000 (28%)\n",
    "* Using ReLU gives more performance over Sigmoid when run with same number of epochs.\n",
    "* As the gradient in case of sigmoid, the gradient decreases very rapidly as input to sigmoid increases, whereas in the case of ReLU, the gradient is 1 and it doesn't decrease with increasing value of input. Hence sigmoid needs more epochs to give better performance when compared with ReLU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
