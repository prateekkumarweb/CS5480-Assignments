{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.380068\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.604063\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.144679\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.646343\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.622206\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.503792\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.485782\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.316346\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.415485\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.437672\n",
      "\n",
      "Train set: Average loss: 0.1498, Accuracy: 58149/60000 (97%)\n",
      "Test set: Average loss: 0.1395, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.380960\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.271658\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.280486\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.202663\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.234569\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.285311\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.193258\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.108683\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.205342\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.162852\n",
      "\n",
      "Train set: Average loss: 0.0758, Accuracy: 58820/60000 (98%)\n",
      "Test set: Average loss: 0.0717, Accuracy: 9811/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.118417\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.114637\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.134752\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.160726\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.095015\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.082332\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.080058\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.193752\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.069902\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.108127\n",
      "\n",
      "Train set: Average loss: 0.0561, Accuracy: 59084/60000 (98%)\n",
      "Test set: Average loss: 0.0530, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.084412\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.213108\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.167835\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.159967\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.061749\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.093129\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.111183\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.091349\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.066639\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.066987\n",
      "\n",
      "Train set: Average loss: 0.0485, Accuracy: 59196/60000 (99%)\n",
      "Test set: Average loss: 0.0475, Accuracy: 9855/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.047873\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.083527\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.069991\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.083318\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.129925\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.156252\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.088632\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.174173\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.213323\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.148449\n",
      "\n",
      "Train set: Average loss: 0.0404, Accuracy: 59302/60000 (99%)\n",
      "Test set: Average loss: 0.0400, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.041207\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.116118\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.067573\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.042045\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.124533\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.046325\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.082490\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.157978\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.100037\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.066267\n",
      "\n",
      "Train set: Average loss: 0.0365, Accuracy: 59385/60000 (99%)\n",
      "Test set: Average loss: 0.0378, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.194660\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.093242\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.078293\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.055372\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.120925\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.192983\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.088914\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.117238\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.031112\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.121706\n",
      "\n",
      "Train set: Average loss: 0.0344, Accuracy: 59436/60000 (99%)\n",
      "Test set: Average loss: 0.0362, Accuracy: 9892/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.095330\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.094267\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.083190\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.062383\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.076615\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.100657\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.102003\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.187427\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.095880\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.069687\n",
      "\n",
      "Train set: Average loss: 0.0314, Accuracy: 59443/60000 (99%)\n",
      "Test set: Average loss: 0.0325, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.086361\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.025657\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.055218\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.106969\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.035941\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.088540\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.089312\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.136563\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.095221\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.094574\n",
      "\n",
      "Train set: Average loss: 0.0317, Accuracy: 59425/60000 (99%)\n",
      "Test set: Average loss: 0.0344, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.017536\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.077813\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.037297\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.069877\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.261507\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.136274\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.016497\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.031853\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.239559\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.119619\n",
      "\n",
      "Train set: Average loss: 0.0270, Accuracy: 59520/60000 (99%)\n",
      "Test set: Average loss: 0.0311, Accuracy: 9903/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "args_batch_size = 64\n",
    "args_test_batch_size = 1000\n",
    "args_epochs = 10\n",
    "args_lr = 0.01\n",
    "args_momentum = 0.5\n",
    "args_no_cuda = False\n",
    "args_seed = 1\n",
    "args_log_interval = 100\n",
    "\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args_seed)\n",
    "if args_cuda:\n",
    "    torch.cuda.manual_seed(args_seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,p):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.batch1 = nn.BatchNorm2d(10)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.batch2 = nn.BatchNorm2d(20)\n",
    "        self.conv2_drop = nn.Dropout2d(p=p)\n",
    "        self.p = p\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.batch3 = nn.BatchNorm1d(50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch1(F.max_pool2d(self.conv1(x), 2)))\n",
    "        x = F.relu(self.batch2(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.batch3(self.fc1(x)))\n",
    "        x = F.dropout(x, training=self.training,p=self.p)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net(p=0.25)\n",
    "if args_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args_lr, momentum=args_momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for data, target in train_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        train_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        train_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "    for data, target in test_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        test_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset),\n",
    "        100. * train_correct / len(train_loader.dataset)))\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, test_correct, len(test_loader.dataset),\n",
    "        100. * test_correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, args_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.356765\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.303545\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.798160\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.355376\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.308903\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.259894\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.233168\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.202254\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.190336\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.202416\n",
      "\n",
      "Train set: Average loss: 0.1138, Accuracy: 58708/60000 (98%)\n",
      "Test set: Average loss: 0.1098, Accuracy: 9787/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.119521\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.232808\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.068229\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.071935\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.161382\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.095637\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.180780\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.136518\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.083266\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.105365\n",
      "\n",
      "Train set: Average loss: 0.0640, Accuracy: 59163/60000 (99%)\n",
      "Test set: Average loss: 0.0620, Accuracy: 9860/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.092156\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.099079\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.056840\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.087688\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.040850\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.121646\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.012603\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.070412\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.036132\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.051832\n",
      "\n",
      "Train set: Average loss: 0.0476, Accuracy: 59330/60000 (99%)\n",
      "Test set: Average loss: 0.0504, Accuracy: 9864/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.018090\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.040653\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.015952\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.018686\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.068481\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.012676\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.036080\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.058683\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.031410\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.067990\n",
      "\n",
      "Train set: Average loss: 0.0369, Accuracy: 59479/60000 (99%)\n",
      "Test set: Average loss: 0.0419, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.035676\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.031947\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.039892\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.062080\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.068555\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.016121\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.024987\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.014821\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.033479\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.021060\n",
      "\n",
      "Train set: Average loss: 0.0299, Accuracy: 59580/60000 (99%)\n",
      "Test set: Average loss: 0.0353, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.059583\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.014516\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.035758\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.029018\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.009995\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.016873\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.012284\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.020681\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.038110\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.031280\n",
      "\n",
      "Train set: Average loss: 0.0251, Accuracy: 59627/60000 (99%)\n",
      "Test set: Average loss: 0.0312, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.029200\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.083466\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.067967\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.016790\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.042544\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.054745\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.010682\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.020241\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.047107\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.027475\n",
      "\n",
      "Train set: Average loss: 0.0219, Accuracy: 59698/60000 (99%)\n",
      "Test set: Average loss: 0.0286, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.017215\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.011061\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.025492\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.029999\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.013943\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.020601\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.049634\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.018020\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.025277\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.036865\n",
      "\n",
      "Train set: Average loss: 0.0210, Accuracy: 59709/60000 (100%)\n",
      "Test set: Average loss: 0.0314, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.044127\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.041209\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.017880\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.024348\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.006292\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.004413\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.021454\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.007746\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.019236\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.044081\n",
      "\n",
      "Train set: Average loss: 0.0186, Accuracy: 59764/60000 (100%)\n",
      "Test set: Average loss: 0.0296, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.010968\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.009927\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.036557\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.025735\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.004579\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.006828\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.026327\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.008310\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.017968\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.077118\n",
      "\n",
      "Train set: Average loss: 0.0179, Accuracy: 59747/60000 (100%)\n",
      "Test set: Average loss: 0.0292, Accuracy: 9900/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "args_batch_size = 64\n",
    "args_test_batch_size = 1000\n",
    "args_epochs = 10\n",
    "args_lr = 0.01\n",
    "args_momentum = 0.5\n",
    "args_no_cuda = False\n",
    "args_seed = 1\n",
    "args_log_interval = 100\n",
    "\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args_seed)\n",
    "if args_cuda:\n",
    "    torch.cuda.manual_seed(args_seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data_mnist', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args_test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.batch1 = nn.BatchNorm2d(10)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.batch2 = nn.BatchNorm2d(20)\n",
    "        # self.conv2_drop = nn.Dropout2d(p=p)\n",
    "        # self.p = p\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.batch3 = nn.BatchNorm1d(50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch1(F.max_pool2d(self.conv1(x), 2)))\n",
    "        x = F.relu(self.batch2(F.max_pool2d(self.conv2(x), 2)))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.batch3(self.fc1(x)))\n",
    "        # x = F.dropout(x, training=self.training,p=self.p)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "if args_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args_lr, momentum=args_momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for data, target in train_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        train_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        train_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "    for data, target in test_loader:\n",
    "        if args_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        test_correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, train_correct, len(train_loader.dataset),\n",
    "        100. * train_correct / len(train_loader.dataset)))\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, test_correct, len(test_loader.dataset),\n",
    "        100. * test_correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, args_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "* Using Batch Normalization with dropout:\n",
    "  * Train performance: `59747/60000 (100%)`\n",
    "  * Test performance: `9900/10000 (99%)`\n",
    "* Using batch normalization has increased the performance. This is expected as normalizing helps learn faster and gives better performance.\n",
    "* Using Batch Normalization without dropout:\n",
    "  * Train performance: `59520/60000 (99%)`\n",
    "  * Test performance: `9903/10000 (99%)`\n",
    "* In this case with batch normalization, with dropout performs better than without dropout, whereas in case of test performace, they give similar performance.\n",
    "* Batch normalization with proper dropout probability is expected to perform better. In this case, if we had the probability that has maximum performace, the model would have learnt better. (We took probability with maximun performance among only 4 values, more should have been tried.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
